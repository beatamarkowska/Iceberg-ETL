{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40d3f89f-4de7-408e-9089-fd1f811187af",
      "metadata": {
        "id": "40d3f89f-4de7-408e-9089-fd1f811187af"
      },
      "source": [
        "Optimized Notebook â€“ Performance Optimization & Exporting Data in WherobotsDB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd31f33-6128-4998-bed0-c9580d7abe4b",
      "metadata": {
        "id": "4bd31f33-6128-4998-bed0-c9580d7abe4b"
      },
      "source": [
        "This notebook extracts, transforms, and loads (ETL) large-scale GDELT event data into WherobotsDB before exporting it as an optimized GeoParquet dataset.\n",
        "\n",
        "ðŸ”¹ Workflow OverviewÂ¶\n",
        "1ï¸âƒ£ Create an optimized table in WherobotsDB (Iceberg format).\n",
        "2ï¸âƒ£ Organize data by GeoHash for spatial indexing.\n",
        "3ï¸âƒ£ Write the structured dataset as a GeoParquet file for efficient querying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9aa6779-f0c2-489c-8301-fa4bb6a1948e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:04:13.353199Z",
          "iopub.status.busy": "2025-04-16T21:04:13.353024Z",
          "iopub.status.idle": "2025-04-16T21:04:40.100467Z",
          "shell.execute_reply": "2025-04-16T21:04:40.100049Z",
          "shell.execute_reply.started": "2025-04-16T21:04:13.353184Z"
        },
        "id": "c9aa6779-f0c2-489c-8301-fa4bb6a1948e",
        "outputId": "592201bc-a161-46c1-c99b-d5ebb9a2b42d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "from sedona.spark import SedonaContext\n",
        "\n",
        "config = SedonaContext.builder() \\\n",
        "    .config(\n",
        "        \"spark.hadoop.fs.s3a.bucket.gdelt-open-data.aws.credentials.provider\",\n",
        "        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sedona = SedonaContext.create(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d4c9567-d0e5-4039-abba-d4856a665c9a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:04:53.128082Z",
          "iopub.status.busy": "2025-04-16T21:04:53.127856Z",
          "iopub.status.idle": "2025-04-16T21:04:53.131652Z",
          "shell.execute_reply": "2025-04-16T21:04:53.131295Z",
          "shell.execute_reply.started": "2025-04-16T21:04:53.128062Z"
        },
        "id": "1d4c9567-d0e5-4039-abba-d4856a665c9a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "csv_path = 's3://gdelt-open-data/events/*.*.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "722587d2-4708-4d05-8293-878dbdf0e8eb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:05:21.472385Z",
          "iopub.status.busy": "2025-04-16T21:05:21.472135Z",
          "iopub.status.idle": "2025-04-16T21:06:27.229519Z",
          "shell.execute_reply": "2025-04-16T21:06:27.228823Z",
          "shell.execute_reply.started": "2025-04-16T21:05:21.472369Z"
        },
        "id": "722587d2-4708-4d05-8293-878dbdf0e8eb",
        "outputId": "dc8ca3a2-4b31-4b9b-c8f5-4e659956c573"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "csv_df = sedona.read.format(\"csv\") \\\n",
        "    .option(\"delimiter\", \"\\\\t\") \\\n",
        "    .load(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6397e79d-368e-4eac-afce-e43745971add",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:07:50.929754Z",
          "iopub.status.busy": "2025-04-16T21:07:50.929503Z",
          "iopub.status.idle": "2025-04-16T21:14:13.956704Z",
          "shell.execute_reply": "2025-04-16T21:14:13.956198Z",
          "shell.execute_reply.started": "2025-04-16T21:07:50.929738Z"
        },
        "id": "6397e79d-368e-4eac-afce-e43745971add",
        "outputId": "0a3aa9e6-bb10-4e6c-a21a-5c73eef7392f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "383958288"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "csv_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc11055-921c-4e55-a98b-dab9061bbf93",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:14:20.732385Z",
          "iopub.status.busy": "2025-04-16T21:14:20.732131Z",
          "iopub.status.idle": "2025-04-16T21:14:21.331006Z",
          "shell.execute_reply": "2025-04-16T21:14:21.329913Z",
          "shell.execute_reply.started": "2025-04-16T21:14:20.732369Z"
        },
        "id": "bcc11055-921c-4e55-a98b-dab9061bbf93"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "response = requests.get('https://gdeltproject.org/data/lookups/CSV.header.dailyupdates.txt')\n",
        "response.raise_for_status()\n",
        "\n",
        "\n",
        "header_line = response.text.splitlines()[0].strip()\n",
        "headers = header_line.split('\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82a2352b-9f5a-4fd9-910e-484ba3523ef0",
      "metadata": {
        "id": "82a2352b-9f5a-4fd9-910e-484ba3523ef0"
      },
      "source": [
        "csv_df.toDF(*headers) zmienia domyÅ›lne nazwy kolumn na bardziej czytelne, zgodnie z listÄ… headers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618ba142-ac64-45e2-9a44-e2b92b3bc5b0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:14:24.004666Z",
          "iopub.status.busy": "2025-04-16T21:14:24.004412Z",
          "iopub.status.idle": "2025-04-16T21:14:24.030089Z",
          "shell.execute_reply": "2025-04-16T21:14:24.029519Z",
          "shell.execute_reply.started": "2025-04-16T21:14:24.004650Z"
        },
        "id": "618ba142-ac64-45e2-9a44-e2b92b3bc5b0"
      },
      "outputs": [],
      "source": [
        "\n",
        "csv_df = csv_df.toDF(*headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d3179d-8439-44d8-8861-fea772485bab",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:14:25.683143Z",
          "iopub.status.busy": "2025-04-16T21:14:25.682905Z",
          "iopub.status.idle": "2025-04-16T21:14:25.688452Z",
          "shell.execute_reply": "2025-04-16T21:14:25.688034Z",
          "shell.execute_reply.started": "2025-04-16T21:14:25.683128Z"
        },
        "id": "88d3179d-8439-44d8-8861-fea772485bab",
        "outputId": "8b08244f-2e2d-4111-d1a0-cc9bb39015a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- GLOBALEVENTID: string (nullable = true)\n",
            " |-- SQLDATE: string (nullable = true)\n",
            " |-- MonthYear: string (nullable = true)\n",
            " |-- Year: string (nullable = true)\n",
            " |-- FractionDate: string (nullable = true)\n",
            " |-- Actor1Code: string (nullable = true)\n",
            " |-- Actor1Name: string (nullable = true)\n",
            " |-- Actor1CountryCode: string (nullable = true)\n",
            " |-- Actor1KnownGroupCode: string (nullable = true)\n",
            " |-- Actor1EthnicCode: string (nullable = true)\n",
            " |-- Actor1Religion1Code: string (nullable = true)\n",
            " |-- Actor1Religion2Code: string (nullable = true)\n",
            " |-- Actor1Type1Code: string (nullable = true)\n",
            " |-- Actor1Type2Code: string (nullable = true)\n",
            " |-- Actor1Type3Code: string (nullable = true)\n",
            " |-- Actor2Code: string (nullable = true)\n",
            " |-- Actor2Name: string (nullable = true)\n",
            " |-- Actor2CountryCode: string (nullable = true)\n",
            " |-- Actor2KnownGroupCode: string (nullable = true)\n",
            " |-- Actor2EthnicCode: string (nullable = true)\n",
            " |-- Actor2Religion1Code: string (nullable = true)\n",
            " |-- Actor2Religion2Code: string (nullable = true)\n",
            " |-- Actor2Type1Code: string (nullable = true)\n",
            " |-- Actor2Type2Code: string (nullable = true)\n",
            " |-- Actor2Type3Code: string (nullable = true)\n",
            " |-- IsRootEvent: string (nullable = true)\n",
            " |-- EventCode: string (nullable = true)\n",
            " |-- EventBaseCode: string (nullable = true)\n",
            " |-- EventRootCode: string (nullable = true)\n",
            " |-- QuadClass: string (nullable = true)\n",
            " |-- GoldsteinScale: string (nullable = true)\n",
            " |-- NumMentions: string (nullable = true)\n",
            " |-- NumSources: string (nullable = true)\n",
            " |-- NumArticles: string (nullable = true)\n",
            " |-- AvgTone: string (nullable = true)\n",
            " |-- Actor1Geo_Type: string (nullable = true)\n",
            " |-- Actor1Geo_FullName: string (nullable = true)\n",
            " |-- Actor1Geo_CountryCode: string (nullable = true)\n",
            " |-- Actor1Geo_ADM1Code: string (nullable = true)\n",
            " |-- Actor1Geo_Lat: string (nullable = true)\n",
            " |-- Actor1Geo_Long: string (nullable = true)\n",
            " |-- Actor1Geo_FeatureID: string (nullable = true)\n",
            " |-- Actor2Geo_Type: string (nullable = true)\n",
            " |-- Actor2Geo_FullName: string (nullable = true)\n",
            " |-- Actor2Geo_CountryCode: string (nullable = true)\n",
            " |-- Actor2Geo_ADM1Code: string (nullable = true)\n",
            " |-- Actor2Geo_Lat: string (nullable = true)\n",
            " |-- Actor2Geo_Long: string (nullable = true)\n",
            " |-- Actor2Geo_FeatureID: string (nullable = true)\n",
            " |-- ActionGeo_Type: string (nullable = true)\n",
            " |-- ActionGeo_FullName: string (nullable = true)\n",
            " |-- ActionGeo_CountryCode: string (nullable = true)\n",
            " |-- ActionGeo_ADM1Code: string (nullable = true)\n",
            " |-- ActionGeo_Lat: string (nullable = true)\n",
            " |-- ActionGeo_Long: string (nullable = true)\n",
            " |-- ActionGeo_FeatureID: string (nullable = true)\n",
            " |-- DATEADDED: string (nullable = true)\n",
            " |-- SOURCEURL: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "csv_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d7240c3-021d-4237-8c41-51a1a9d0dd33",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:16:38.751946Z",
          "iopub.status.busy": "2025-04-16T21:16:38.751747Z",
          "iopub.status.idle": "2025-04-16T21:16:38.775833Z",
          "shell.execute_reply": "2025-04-16T21:16:38.775318Z",
          "shell.execute_reply.started": "2025-04-16T21:16:38.751932Z"
        },
        "id": "3d7240c3-021d-4237-8c41-51a1a9d0dd33"
      },
      "outputs": [],
      "source": [
        "\n",
        "csv_df.createOrReplaceTempView('csv_df')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14aeb16-7fc3-4920-aca4-61adae1f5a62",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:16:39.533778Z",
          "iopub.status.busy": "2025-04-16T21:16:39.533524Z",
          "iopub.status.idle": "2025-04-16T21:16:39.536606Z",
          "shell.execute_reply": "2025-04-16T21:16:39.536156Z",
          "shell.execute_reply.started": "2025-04-16T21:16:39.533763Z"
        },
        "id": "d14aeb16-7fc3-4920-aca4-61adae1f5a62"
      },
      "outputs": [],
      "source": [
        "name = 'beatka'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98f45db-75d4-4756-9b29-340c7ba62f08",
      "metadata": {
        "id": "b98f45db-75d4-4756-9b29-340c7ba62f08"
      },
      "source": [
        "ðŸ”¸ CREATE OR REPLACE TABLE wherobots.{name}.gdelt AS\n",
        "Tworzy nowÄ… tabelÄ™ w katalogu Spark SQL (tutaj: wherobots.{name}.gdelt). JeÅ›li tabela juÅ¼ istnieje, zostanie nadpisana.\n",
        "\n",
        "name to zmienna w Pythonie â€” np. moÅ¼e byÄ‡ name = \"example_user\"\n",
        "\n",
        "ðŸ”¸ FROM csv_df\n",
        "Å¹rÃ³dÅ‚o danych to DataFrame csv_df â€” czyli najpewniej dane z pliku CSV.\n",
        "\n",
        "ðŸ”¸ SELECT *, ST_SetSRID(ST_Point(ActionGeo_Long, ActionGeo_Lat), 4326) as geometry\n",
        "SELECT *: wybiera wszystkie kolumny z csv_df\n",
        "\n",
        "ST_Point(long, lat): tworzy punkt geometryczny z wartoÅ›ci dÅ‚ugoÅ›ci i szerokoÅ›ci geograficznej\n",
        "\n",
        "ST_SetSRID(..., 4326): ustawia ukÅ‚ad wspÃ³Å‚rzÄ™dnych (CRS) â€” tutaj EPSG:4326, czyli standardowy ukÅ‚ad geograficzny WGS 84\n",
        "\n",
        "as geometry: nadaje nazwÄ™ nowej kolumnie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a67f24c-dfb2-44f8-a7a9-482ad8815cda",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:16:40.886550Z",
          "iopub.status.busy": "2025-04-16T21:16:40.886326Z",
          "iopub.status.idle": "2025-04-16T21:27:17.847962Z",
          "shell.execute_reply": "2025-04-16T21:27:17.847416Z",
          "shell.execute_reply.started": "2025-04-16T21:16:40.886535Z"
        },
        "id": "4a67f24c-dfb2-44f8-a7a9-482ad8815cda",
        "outputId": "dfbbbc03-7fe2-4fff-c14d-57603046dc5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the Havasu table and create a geometry\n",
        "sedona.sql(f'''\n",
        "CREATE OR REPLACE TABLE wherobots.{name}.gdelt AS\n",
        "SELECT *,\n",
        "ST_SetSRID(\n",
        "    ST_Point(ActionGeo_Long, ActionGeo_Lat),\n",
        "4326) as geometry\n",
        "FROM csv_df\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b1d694-cc98-4f57-b96a-c50f3d42c73d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:28:45.060930Z",
          "iopub.status.busy": "2025-04-16T21:28:45.060612Z",
          "iopub.status.idle": "2025-04-16T21:28:45.064317Z",
          "shell.execute_reply": "2025-04-16T21:28:45.063784Z",
          "shell.execute_reply.started": "2025-04-16T21:28:45.060914Z"
        },
        "id": "84b1d694-cc98-4f57-b96a-c50f3d42c73d"
      },
      "outputs": [],
      "source": [
        "# Save the JSON file for the 4326 projection for the GeoParquet metadata\n",
        "\n",
        "projjson = '''{\n",
        "    \"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\",\n",
        "    \"type\": \"GeographicCRS\",\n",
        "    \"name\": \"WGS 84\",\n",
        "    \"datum_ensemble\": {\n",
        "        \"name\": \"World Geodetic System 1984 ensemble\",\n",
        "        \"members\": [\n",
        "            {\n",
        "                \"name\": \"World Geodetic System 1984 (Transit)\",\n",
        "                \"id\": {\n",
        "                    \"authority\": \"EPSG\",\n",
        "                    \"code\": 1166\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"World Geodetic System 1984 (G730)\",\n",
        "                \"id\": {\n",
        "                    \"authority\": \"EPSG\",\n",
        "                    \"code\": 1152\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"World Geodetic System 1984 (G873)\",\n",
        "                \"id\": {\n",
        "                    \"authority\": \"EPSG\",\n",
        "                    \"code\": 1153\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"World Geodetic System 1984 (G1150)\",\n",
        "                \"id\": {\n",
        "                    \"authority\": \"EPSG\",\n",
        "                    \"code\": 1154\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"World Geodetic System 1984 (G1674)\",\n",
        "                \"id\": {\n",
        "                    \"authority\": \"EPSG\",\n",
        "                    \"code\": 1155\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"World Geodetic System 1984 (G1762)\",\n",
        "                \"id\": {\n",
        "                    \"authority\": \"EPSG\",\n",
        "                    \"code\": 1156\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"World Geodetic System 1984 (G2139)\",\n",
        "                \"id\": {\n",
        "                    \"authority\": \"EPSG\",\n",
        "                    \"code\": 1309\n",
        "                }\n",
        "            }\n",
        "        ],\n",
        "        \"ellipsoid\": {\n",
        "            \"name\": \"WGS 84\",\n",
        "            \"semi_major_axis\": 6378137,\n",
        "            \"inverse_flattening\": 298.257223563\n",
        "        },\n",
        "        \"accuracy\": \"2.0\",\n",
        "        \"id\": {\n",
        "            \"authority\": \"EPSG\",\n",
        "            \"code\": 6326\n",
        "        }\n",
        "    },\n",
        "    \"coordinate_system\": {\n",
        "        \"subtype\": \"ellipsoidal\",\n",
        "        \"axis\": [\n",
        "            {\n",
        "                \"name\": \"Geodetic latitude\",\n",
        "                \"abbreviation\": \"Lat\",\n",
        "                \"direction\": \"north\",\n",
        "                \"unit\": \"degree\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"Geodetic longitude\",\n",
        "                \"abbreviation\": \"Lon\",\n",
        "                \"direction\": \"east\",\n",
        "                \"unit\": \"degree\"\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"scope\": \"Horizontal component of 3D system.\",\n",
        "    \"area\": \"World.\",\n",
        "    \"bbox\": {\n",
        "        \"south_latitude\": -90,\n",
        "        \"west_longitude\": -180,\n",
        "        \"north_latitude\": 90,\n",
        "        \"east_longitude\": 180\n",
        "    },\n",
        "    \"id\": {\n",
        "        \"authority\": \"EPSG\",\n",
        "        \"code\": 4326\n",
        "    }\n",
        "}'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28edf775-baf5-4222-a1bc-28180b37e973",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:33:39.763869Z",
          "iopub.status.busy": "2025-04-16T21:33:39.763650Z",
          "iopub.status.idle": "2025-04-16T21:33:39.767460Z",
          "shell.execute_reply": "2025-04-16T21:33:39.766652Z",
          "shell.execute_reply.started": "2025-04-16T21:33:39.763853Z"
        },
        "id": "28edf775-baf5-4222-a1bc-28180b37e973"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "user_uri = os.getenv(\"USER_S3_PATH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8283b84f-407f-4eb6-8619-8d4e0a10af48",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:33:40.827623Z",
          "iopub.status.busy": "2025-04-16T21:33:40.827099Z",
          "iopub.status.idle": "2025-04-16T21:33:41.059828Z",
          "shell.execute_reply": "2025-04-16T21:33:41.059415Z",
          "shell.execute_reply.started": "2025-04-16T21:33:40.827607Z"
        },
        "id": "8283b84f-407f-4eb6-8619-8d4e0a10af48"
      },
      "outputs": [],
      "source": [
        "\n",
        "gdelt = sedona.sql(f'''SELECT\n",
        "*,\n",
        "ST_GeoHash(geometry, 15) AS geohash,\n",
        "struct(st_xmin(geometry) as xmin, st_ymin(geometry) as ymin, st_xmax(geometry) as xmax, st_ymax(geometry) as ymax) as bbox\n",
        "FROM wherobots.{name}.gdelt''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b82bb9-5079-4d7f-89b9-f14771645727",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-16T21:33:42.032635Z",
          "iopub.status.busy": "2025-04-16T21:33:42.032325Z",
          "iopub.status.idle": "2025-04-16T21:43:55.981675Z",
          "shell.execute_reply": "2025-04-16T21:43:55.981304Z",
          "shell.execute_reply.started": "2025-04-16T21:33:42.032618Z"
        },
        "id": "64b82bb9-5079-4d7f-89b9-f14771645727",
        "outputId": "1829666a-b938-4be1-e5c9-f3eed15a291d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/16 21:43:55 ERROR TaskSetManager: Task 141 in stage 11.0 failed 4 times; aborting job\n",
            "[Stage 11:====================================>                 (141 + 4) / 209]\r"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o174.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 141 in stage 11.0 failed 4 times, most recent failure: Lost task 141.3 in stage 11.0 (TID 8438) (10.1.87.63 executor 1): java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:372)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:221)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:205)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:221)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:199)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:334)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:372)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:221)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:205)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:221)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:199)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:334)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[0;32m<timed eval>:9\u001b[0m\n",
            "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[0;32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o174.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 141 in stage 11.0 failed 4 times, most recent failure: Lost task 141.3 in stage 11.0 (TID 8438) (10.1.87.63 executor 1): java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:372)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:221)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:205)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:221)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:199)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:334)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:372)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:221)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:205)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:221)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:199)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:112)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:334)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 11:====================================>                 (141 + 1) / 209]\r"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "gdelt.repartitionByRange(30, \"geohash\") \\\n",
        "    .sortWithinPartitions(\"geohash\") \\\n",
        "    .drop(\"geohash15\") \\\n",
        "    .write \\\n",
        "    .format(\"geoparquet\") \\\n",
        "    .option(\"geoparquet.version\", \"1.1.0\") \\\n",
        "    .option(\"geoparquet.covering\", \"bbox\") \\\n",
        "    .option(\"geoparquet.crs\", projjson) \\\n",
        "    .save(user_uri + \"gdelt-snappy\", mode='overwrite', compression='snappy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91552779-4967-44e9-993a-92f07a285b9b",
      "metadata": {
        "id": "91552779-4967-44e9-993a-92f07a285b9b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}